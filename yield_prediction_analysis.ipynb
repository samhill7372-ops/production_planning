{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Planning - Material Yield Prediction System\n",
    "\n",
    "## SAP Manufacturing Logic\n",
    "- **101** = INPUT material consumption (raw materials, BFIN)\n",
    "- **261** = OUTPUT material production (finished goods, BFOUT derived from dimensions)\n",
    "- Input and Output materials are **DIFFERENT**\n",
    "- Join **ONLY** on `MANUFACTURINGORDER`\n",
    "- `Yield = Total_Output_BF / Total_Input_BF`\n",
    "\n",
    "## Real-World Use Cases\n",
    "1. **Forward Planning**: \"If I consume X BF of raw material, how much output will I get?\"\n",
    "2. **Reverse Planning**: \"If I need Y BF of finished goods, how much raw material do I need?\"\n",
    "3. **Material Selection**: \"Which raw material gives the best yield for my needs?\"\n",
    "4. **Anomaly Detection**: \"Is this manufacturing order producing abnormal loss?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "df_101 = pd.read_csv('../101.csv')\n",
    "df_261 = pd.read_csv('../261.csv')\n",
    "\n",
    "print(f\"101.csv (Inputs): {df_101.shape[0]:,} rows, {df_101.shape[1]} columns\")\n",
    "print(f\"261.csv (Outputs): {df_261.shape[0]:,} rows, {df_261.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect data structure\n",
    "print(\"=== 101.csv (Input Materials) ===\")\n",
    "print(df_101.dtypes)\n",
    "print(f\"\\nMissing values:\\n{df_101.isnull().sum()}\")\n",
    "print(f\"\\n=== 261.csv (Output Materials) ===\")\n",
    "print(df_261.dtypes)\n",
    "print(f\"\\nMissing values:\\n{df_261.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Cleaning Steps\n",
    "1. Remove deleted records (`is_deleted = TRUE`)\n",
    "2. Convert data types (dates, numerics)\n",
    "3. Handle missing values\n",
    "4. Remove duplicates\n",
    "5. Validate data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df, name):\n",
    "    \"\"\"Clean a single dataframe with standard preprocessing steps.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Cleaning {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # 1. Remove deleted records\n",
    "    # Handle both boolean and string representations\n",
    "    if 'is_deleted' in df.columns:\n",
    "        df['is_deleted'] = df['is_deleted'].astype(str).str.upper()\n",
    "        deleted_count = (df['is_deleted'] == 'TRUE').sum()\n",
    "        df = df[df['is_deleted'] != 'TRUE'].copy()\n",
    "        print(f\"1. Removed {deleted_count:,} deleted records\")\n",
    "    \n",
    "    # 2. Convert POSTINGDATE to datetime\n",
    "    if 'POSTINGDATE' in df.columns:\n",
    "        df['POSTINGDATE'] = pd.to_datetime(df['POSTINGDATE'], errors='coerce')\n",
    "        invalid_dates = df['POSTINGDATE'].isnull().sum()\n",
    "        print(f\"2. Converted POSTINGDATE to datetime ({invalid_dates} invalid dates)\")\n",
    "    \n",
    "    # 3. Convert numeric columns\n",
    "    numeric_cols = ['MATERIALTHICKNESS', 'TALLYLENGTH', 'TALLYWIDTH', 'BFIN', 'BFOUT']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    print(f\"3. Converted numeric columns: {[c for c in numeric_cols if c in df.columns]}\")\n",
    "    \n",
    "    # 4. Handle missing values in key columns\n",
    "    key_cols = ['MANUFACTURINGORDER', 'PLANT', 'MATERIAL']\n",
    "    missing_key = df[key_cols].isnull().any(axis=1).sum()\n",
    "    df = df.dropna(subset=key_cols)\n",
    "    print(f\"4. Removed {missing_key:,} rows with missing key columns\")\n",
    "    \n",
    "    # 5. Remove duplicates\n",
    "    dup_count = df.duplicated().sum()\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"5. Removed {dup_count:,} duplicate rows\")\n",
    "    \n",
    "    # 6. Remove rows with negative BF values\n",
    "    bf_cols = ['BFIN', 'BFOUT']\n",
    "    for col in bf_cols:\n",
    "        if col in df.columns:\n",
    "            negative_count = (df[col] < 0).sum()\n",
    "            df = df[df[col] >= 0]\n",
    "            if negative_count > 0:\n",
    "                print(f\"6. Removed {negative_count:,} rows with negative {col}\")\n",
    "    \n",
    "    final_rows = len(df)\n",
    "    print(f\"\\nResult: {initial_rows:,} -> {final_rows:,} rows ({initial_rows - final_rows:,} removed, {(final_rows/initial_rows)*100:.1f}% retained)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply cleaning to both datasets\n",
    "df_101_clean = clean_dataframe(df_101, \"101.csv (Inputs)\")\n",
    "df_261_clean = clean_dataframe(df_261, \"261.csv (Outputs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality summary\n",
    "print(\"=== Data Quality Summary After Cleaning ===\\n\")\n",
    "\n",
    "print(\"101.csv (Inputs):\")\n",
    "print(f\"  Rows: {len(df_101_clean):,}\")\n",
    "print(f\"  Unique Manufacturing Orders: {df_101_clean['MANUFACTURINGORDER'].nunique():,}\")\n",
    "print(f\"  Unique Materials: {df_101_clean['MATERIAL'].nunique():,}\")\n",
    "print(f\"  Unique Plants: {df_101_clean['PLANT'].nunique():,}\")\n",
    "print(f\"  Date Range: {df_101_clean['POSTINGDATE'].min()} to {df_101_clean['POSTINGDATE'].max()}\")\n",
    "\n",
    "print(\"\\n261.csv (Outputs):\")\n",
    "print(f\"  Rows: {len(df_261_clean):,}\")\n",
    "print(f\"  Unique Manufacturing Orders: {df_261_clean['MANUFACTURINGORDER'].nunique():,}\")\n",
    "print(f\"  Unique Materials: {df_261_clean['MATERIAL'].nunique():,}\")\n",
    "print(f\"  Unique Plants: {df_261_clean['PLANT'].nunique():,}\")\n",
    "print(f\"  Date Range: {df_261_clean['POSTINGDATE'].min()} to {df_261_clean['POSTINGDATE'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview cleaned data\n",
    "print(\"=== Sample of Cleaned 101 Data (Inputs) ===\")\n",
    "display(df_101_clean.head(10))\n",
    "\n",
    "print(\"\\n=== Sample of Cleaned 261 Data (Outputs) ===\")\n",
    "display(df_261_clean.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Yield Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# First, merge input and output data to calculate yield\n",
    "# Aggregate input data by manufacturing order\n",
    "input_agg = df_101_clean.groupby('MANUFACTURINGORDER').agg({\n",
    "    'BFIN': 'sum',\n",
    "    'PLANT': 'first',\n",
    "    'MATERIAL': 'first',\n",
    "    'MATERIALTHICKNESS': 'first'\n",
    "}).reset_index()\n",
    "input_agg.columns = ['MANUFACTURINGORDER', 'Total_Input_BF', 'Plant', 'Input_Material', 'Thickness']\n",
    "\n",
    "# Aggregate output data by manufacturing order\n",
    "output_agg = df_261_clean.groupby('MANUFACTURINGORDER').agg({\n",
    "    'BFOUT': 'sum'\n",
    "}).reset_index()\n",
    "output_agg.columns = ['MANUFACTURINGORDER', 'Total_Output_BF']\n",
    "\n",
    "# Merge input and output\n",
    "df = input_agg.merge(output_agg, on='MANUFACTURINGORDER', how='inner')\n",
    "\n",
    "# Calculate yield percentage\n",
    "df['Yield_Percentage'] = (df['Total_Output_BF'] / df['Total_Input_BF']) * 100\n",
    "\n",
    "# Filter out unrealistic yields (e.g., > 100% or negative)\n",
    "df = df[(df['Yield_Percentage'] > 0) & (df['Yield_Percentage'] <= 100)]\n",
    "\n",
    "print(f\"Merged dataset: {len(df):,} manufacturing orders\")\n",
    "print(f\"Yield range: {df['Yield_Percentage'].min():.1f}% - {df['Yield_Percentage'].max():.1f}%\")\n",
    "print(f\"Mean yield: {df['Yield_Percentage'].mean():.1f}%\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "le_plant = LabelEncoder()\n",
    "le_material = LabelEncoder()\n",
    "\n",
    "df['Plant_Encoded'] = le_plant.fit_transform(df['Plant'].astype(str))\n",
    "df['Material_Encoded'] = le_material.fit_transform(df['Input_Material'].astype(str))\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['Total_Input_BF', 'Plant_Encoded', 'Material_Encoded', 'Thickness']\n",
    "X = df[feature_cols].copy()\n",
    "y = df['Yield_Percentage'].copy()\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='r2')\n",
    "print(f\"\\nCross-validation R² scores: {cv_scores.round(4)}\")\n",
    "print(f\"Mean CV R²: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "print(\"=\" * 50)\n",
    "print(\"RANDOM FOREST MODEL EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Training metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "\n",
    "# Test metrics\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  R² Score: {train_r2:.4f}\")\n",
    "print(f\"  MAE: {train_mae:.2f}%\")\n",
    "print(f\"  RMSE: {train_rmse:.2f}%\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  R² Score: {test_r2:.4f}\")\n",
    "print(f\"  MAE: {test_mae:.2f}%\")\n",
    "print(f\"  RMSE: {test_rmse:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "display(importance_df)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.barh(importance_df['Feature'], importance_df['Importance'], color='forestgreen')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Random Forest Feature Importance')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.5, color='forestgreen')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect')\n",
    "axes[0].set_xlabel('Actual Yield %')\n",
    "axes[0].set_ylabel('Predicted Yield %')\n",
    "axes[0].set_title('Actual vs Predicted Yield')\n",
    "axes[0].legend()\n",
    "\n",
    "# Error distribution\n",
    "errors = y_test - y_pred_test\n",
    "axes[1].hist(errors, bins=30, color='forestgreen', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Prediction Error (%)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title(f'Error Distribution (Mean: {errors.mean():.2f}%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict yield for new inputs\n",
    "def predict_yield(input_bf, plant, material, thickness):\n",
    "    \"\"\"Predict yield percentage for given inputs.\"\"\"\n",
    "    # Encode plant and material\n",
    "    try:\n",
    "        plant_enc = le_plant.transform([str(plant)])[0]\n",
    "    except ValueError:\n",
    "        plant_enc = 0  # Unknown plant\n",
    "    \n",
    "    try:\n",
    "        material_enc = le_material.transform([str(material)])[0]\n",
    "    except ValueError:\n",
    "        material_enc = 0  # Unknown material\n",
    "    \n",
    "    # Create input array\n",
    "    X_new = pd.DataFrame([[input_bf, plant_enc, material_enc, thickness]], \n",
    "                         columns=feature_cols)\n",
    "    \n",
    "    # Predict\n",
    "    predicted_yield = rf_model.predict(X_new)[0]\n",
    "    expected_output = input_bf * predicted_yield / 100\n",
    "    \n",
    "    return {\n",
    "        'input_bf': input_bf,\n",
    "        'predicted_yield_pct': predicted_yield,\n",
    "        'expected_output_bf': expected_output\n",
    "    }\n",
    "\n",
    "# Example prediction\n",
    "example = predict_yield(\n",
    "    input_bf=10000,\n",
    "    plant=df['Plant'].iloc[0],\n",
    "    material=df['Input_Material'].iloc[0],\n",
    "    thickness=df['Thickness'].iloc[0]\n",
    ")\n",
    "\n",
    "print(\"Example Prediction:\")\n",
    "print(f\"  Input: {example['input_bf']:,} BF\")\n",
    "print(f\"  Predicted Yield: {example['predicted_yield_pct']:.1f}%\")\n",
    "print(f\"  Expected Output: {example['expected_output_bf']:,.0f} BF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "production_planning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

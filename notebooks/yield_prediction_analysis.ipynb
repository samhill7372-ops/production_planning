{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Planning - Material Yield Prediction System\n",
    "\n",
    "## SAP Manufacturing Logic\n",
    "- **101** = INPUT material consumption (raw materials, BFIN)\n",
    "- **261** = OUTPUT material production (finished goods, BFOUT derived from dimensions)\n",
    "- Input and Output materials are **DIFFERENT**\n",
    "- Join **ONLY** on `MANUFACTURINGORDER`\n",
    "- `Yield = Total_Output_BF / Total_Input_BF`\n",
    "\n",
    "## Real-World Use Cases\n",
    "1. **Forward Planning**: \"If I consume X BF of raw material, how much output will I get?\"\n",
    "2. **Reverse Planning**: \"If I need Y BF of finished goods, how much raw material do I need?\"\n",
    "3. **Material Selection**: \"Which raw material gives the best yield for my needs?\"\n",
    "4. **Anomaly Detection**: \"Is this manufacturing order producing abnormal loss?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "df_101 = pd.read_csv('../101.csv')\n",
    "df_261 = pd.read_csv('../261.csv')\n",
    "\n",
    "print(f\"101.csv (Inputs): {df_101.shape[0]:,} rows, {df_101.shape[1]} columns\")\n",
    "print(f\"261.csv (Outputs): {df_261.shape[0]:,} rows, {df_261.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect data structure\n",
    "print(\"=== 101.csv (Input Materials) ===\")\n",
    "print(df_101.dtypes)\n",
    "print(f\"\\nMissing values:\\n{df_101.isnull().sum()}\")\n",
    "print(f\"\\n=== 261.csv (Output Materials) ===\")\n",
    "print(df_261.dtypes)\n",
    "print(f\"\\nMissing values:\\n{df_261.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Cleaning Steps\n",
    "1. Remove deleted records (`is_deleted = TRUE`)\n",
    "2. Convert data types (dates, numerics)\n",
    "3. Handle missing values\n",
    "4. Remove duplicates\n",
    "5. Validate data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df, name):\n",
    "    \"\"\"Clean a single dataframe with standard preprocessing steps.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Cleaning {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # 1. Remove deleted records\n",
    "    # Handle both boolean and string representations\n",
    "    if 'is_deleted' in df.columns:\n",
    "        df['is_deleted'] = df['is_deleted'].astype(str).str.upper()\n",
    "        deleted_count = (df['is_deleted'] == 'TRUE').sum()\n",
    "        df = df[df['is_deleted'] != 'TRUE'].copy()\n",
    "        print(f\"1. Removed {deleted_count:,} deleted records\")\n",
    "    \n",
    "    # 2. Convert POSTINGDATE to datetime\n",
    "    if 'POSTINGDATE' in df.columns:\n",
    "        df['POSTINGDATE'] = pd.to_datetime(df['POSTINGDATE'], errors='coerce')\n",
    "        invalid_dates = df['POSTINGDATE'].isnull().sum()\n",
    "        print(f\"2. Converted POSTINGDATE to datetime ({invalid_dates} invalid dates)\")\n",
    "    \n",
    "    # 3. Convert numeric columns\n",
    "    numeric_cols = ['MATERIALTHICKNESS', 'TALLYLENGTH', 'TALLYWIDTH', 'BFIN', 'BFOUT']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    print(f\"3. Converted numeric columns: {[c for c in numeric_cols if c in df.columns]}\")\n",
    "    \n",
    "    # 4. Handle missing values in key columns\n",
    "    key_cols = ['MANUFACTURINGORDER', 'PLANT', 'MATERIAL']\n",
    "    missing_key = df[key_cols].isnull().any(axis=1).sum()\n",
    "    df = df.dropna(subset=key_cols)\n",
    "    print(f\"4. Removed {missing_key:,} rows with missing key columns\")\n",
    "    \n",
    "    # 5. Remove duplicates\n",
    "    dup_count = df.duplicated().sum()\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"5. Removed {dup_count:,} duplicate rows\")\n",
    "    \n",
    "    # 6. Remove rows with negative BF values\n",
    "    bf_cols = ['BFIN', 'BFOUT']\n",
    "    for col in bf_cols:\n",
    "        if col in df.columns:\n",
    "            negative_count = (df[col] < 0).sum()\n",
    "            df = df[df[col] >= 0]\n",
    "            if negative_count > 0:\n",
    "                print(f\"6. Removed {negative_count:,} rows with negative {col}\")\n",
    "    \n",
    "    final_rows = len(df)\n",
    "    print(f\"\\nResult: {initial_rows:,} -> {final_rows:,} rows ({initial_rows - final_rows:,} removed, {(final_rows/initial_rows)*100:.1f}% retained)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply cleaning to both datasets\n",
    "df_101_clean = clean_dataframe(df_101, \"101.csv (Inputs)\")\n",
    "df_261_clean = clean_dataframe(df_261, \"261.csv (Outputs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality summary\n",
    "print(\"=== Data Quality Summary After Cleaning ===\\n\")\n",
    "\n",
    "print(\"101.csv (Inputs):\")\n",
    "print(f\"  Rows: {len(df_101_clean):,}\")\n",
    "print(f\"  Unique Manufacturing Orders: {df_101_clean['MANUFACTURINGORDER'].nunique():,}\")\n",
    "print(f\"  Unique Materials: {df_101_clean['MATERIAL'].nunique():,}\")\n",
    "print(f\"  Unique Plants: {df_101_clean['PLANT'].nunique():,}\")\n",
    "print(f\"  Date Range: {df_101_clean['POSTINGDATE'].min()} to {df_101_clean['POSTINGDATE'].max()}\")\n",
    "\n",
    "print(\"\\n261.csv (Outputs):\")\n",
    "print(f\"  Rows: {len(df_261_clean):,}\")\n",
    "print(f\"  Unique Manufacturing Orders: {df_261_clean['MANUFACTURINGORDER'].nunique():,}\")\n",
    "print(f\"  Unique Materials: {df_261_clean['MATERIAL'].nunique():,}\")\n",
    "print(f\"  Unique Plants: {df_261_clean['PLANT'].nunique():,}\")\n",
    "print(f\"  Date Range: {df_261_clean['POSTINGDATE'].min()} to {df_261_clean['POSTINGDATE'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview cleaned data\n",
    "print(\"=== Sample of Cleaned 101 Data (Inputs) ===\")\n",
    "display(df_101_clean.head(10))\n",
    "\n",
    "print(\"\\n=== Sample of Cleaned 261 Data (Outputs) ===\")\n",
    "display(df_261_clean.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Random Forest Yield Prediction",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# First, merge input and output data to calculate yield\n# Aggregate input data by manufacturing order\ninput_agg = df_101_clean.groupby('MANUFACTURINGORDER').agg({\n    'BFIN': 'sum',\n    'PLANT': 'first',\n    'MATERIAL': 'first',\n    'MATERIALTHICKNESS': 'first'\n}).reset_index()\ninput_agg.columns = ['MANUFACTURINGORDER', 'Total_Input_BF', 'Plant', 'Input_Material', 'Thickness']\n\n# Aggregate output data by manufacturing order\noutput_agg = df_261_clean.groupby('MANUFACTURINGORDER').agg({\n    'BFOUT': 'sum'\n}).reset_index()\noutput_agg.columns = ['MANUFACTURINGORDER', 'Total_Output_BF']\n\n# Merge input and output\ndf = input_agg.merge(output_agg, on='MANUFACTURINGORDER', how='inner')\n\n# Calculate yield percentage\ndf['Yield_Percentage'] = (df['Total_Output_BF'] / df['Total_Input_BF']) * 100\n\n# Filter out unrealistic yields (e.g., > 100% or negative)\ndf = df[(df['Yield_Percentage'] > 0) & (df['Yield_Percentage'] <= 100)]\n\nprint(f\"Merged dataset: {len(df):,} manufacturing orders\")\nprint(f\"Yield range: {df['Yield_Percentage'].min():.1f}% - {df['Yield_Percentage'].max():.1f}%\")\nprint(f\"Mean yield: {df['Yield_Percentage'].mean():.1f}%\")\ndisplay(df.head())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Encode categorical features\nle_plant = LabelEncoder()\nle_material = LabelEncoder()\n\ndf['Plant_Encoded'] = le_plant.fit_transform(df['Plant'].astype(str))\ndf['Material_Encoded'] = le_material.fit_transform(df['Input_Material'].astype(str))\n\n# Prepare features and target\nfeature_cols = ['Total_Input_BF', 'Plant_Encoded', 'Material_Encoded', 'Thickness']\nX = df[feature_cols].copy()\ny = df['Yield_Percentage'].copy()\n\n# Handle missing values\nX = X.fillna(X.median())\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Features: {feature_cols}\")\nprint(f\"Training set: {len(X_train):,} samples\")\nprint(f\"Test set: {len(X_test):,} samples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Train Random Forest model\nrf_model = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=15,\n    min_samples_split=5,\n    min_samples_leaf=2,\n    random_state=42,\n    n_jobs=-1\n)\n\nprint(\"Training Random Forest model...\")\nrf_model.fit(X_train, y_train)\nprint(\"Training complete!\")\n\n# Predictions\ny_pred_train = rf_model.predict(X_train)\ny_pred_test = rf_model.predict(X_test)\n\n# Cross-validation\ncv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='r2')\nprint(f\"\\nCross-validation R² scores: {cv_scores.round(4)}\")\nprint(f\"Mean CV R²: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Model Evaluation\nprint(\"=\" * 50)\nprint(\"RANDOM FOREST MODEL EVALUATION\")\nprint(\"=\" * 50)\n\n# Training metrics\ntrain_r2 = r2_score(y_train, y_pred_train)\ntrain_mae = mean_absolute_error(y_train, y_pred_train)\ntrain_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n\n# Test metrics\ntest_r2 = r2_score(y_test, y_pred_test)\ntest_mae = mean_absolute_error(y_test, y_pred_test)\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\nprint(f\"\\nTraining Set:\")\nprint(f\"  R² Score: {train_r2:.4f}\")\nprint(f\"  MAE: {train_mae:.2f}%\")\nprint(f\"  RMSE: {train_rmse:.2f}%\")\n\nprint(f\"\\nTest Set:\")\nprint(f\"  R² Score: {test_r2:.4f}\")\nprint(f\"  MAE: {test_mae:.2f}%\")\nprint(f\"  RMSE: {test_rmse:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Feature Importance\nimportance_df = pd.DataFrame({\n    'Feature': feature_cols,\n    'Importance': rf_model.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(\"Feature Importance:\")\ndisplay(importance_df)\n\n# Plot\nfig, ax = plt.subplots(figsize=(8, 5))\nax.barh(importance_df['Feature'], importance_df['Importance'], color='forestgreen')\nax.set_xlabel('Importance')\nax.set_title('Random Forest Feature Importance')\nax.invert_yaxis()\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Actual vs Predicted Plot\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Scatter plot\naxes[0].scatter(y_test, y_pred_test, alpha=0.5, color='forestgreen')\naxes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect')\naxes[0].set_xlabel('Actual Yield %')\naxes[0].set_ylabel('Predicted Yield %')\naxes[0].set_title('Actual vs Predicted Yield')\naxes[0].legend()\n\n# Error distribution\nerrors = y_test - y_pred_test\naxes[1].hist(errors, bins=30, color='forestgreen', edgecolor='black', alpha=0.7)\naxes[1].axvline(0, color='red', linestyle='--')\naxes[1].set_xlabel('Prediction Error (%)')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title(f'Error Distribution (Mean: {errors.mean():.2f}%)')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Function to predict yield for new inputs\ndef predict_yield(input_bf, plant, material, thickness):\n    \"\"\"Predict yield percentage for given inputs.\"\"\"\n    # Encode plant and material\n    try:\n        plant_enc = le_plant.transform([str(plant)])[0]\n    except ValueError:\n        plant_enc = 0  # Unknown plant\n    \n    try:\n        material_enc = le_material.transform([str(material)])[0]\n    except ValueError:\n        material_enc = 0  # Unknown material\n    \n    # Create input array\n    X_new = pd.DataFrame([[input_bf, plant_enc, material_enc, thickness]], \n                         columns=feature_cols)\n    \n    # Predict\n    predicted_yield = rf_model.predict(X_new)[0]\n    expected_output = input_bf * predicted_yield / 100\n    \n    return {\n        'input_bf': input_bf,\n        'predicted_yield_pct': predicted_yield,\n        'expected_output_bf': expected_output\n    }\n\n# Example prediction\nexample = predict_yield(\n    input_bf=10000,\n    plant=df['Plant'].iloc[0],\n    material=df['Input_Material'].iloc[0],\n    thickness=df['Thickness'].iloc[0]\n)\n\nprint(\"Example Prediction:\")\nprint(f\"  Input: {example['input_bf']:,} BF\")\nprint(f\"  Predicted Yield: {example['predicted_yield_pct']:.1f}%\")\nprint(f\"  Expected Output: {example['expected_output_bf']:,.0f} BF\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "production_planning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}